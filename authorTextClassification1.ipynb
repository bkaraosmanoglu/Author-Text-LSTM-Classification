{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\users\\burak\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\burak\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\burak\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\burak\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\burak\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\burak\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/DeepLearning/Text-Classification-Models/Data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EAP</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HPL</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EAP</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MWS</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HPL</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  author                                               text\n",
       "0    EAP  This process, however, afforded me no means of...\n",
       "1    HPL  It never once occurred to me that the fumbling...\n",
       "2    EAP  In his left hand was a gold snuff box, from wh...\n",
       "3    MWS  How lovely is spring As we looked from Windsor...\n",
       "4    HPL  Finding nothing else, not even gold, the Super..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['EAP', 'HPL', 'MWS'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.author.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19579 entries, 0 to 19578\n",
      "Data columns (total 2 columns):\n",
      "author    19579 non-null object\n",
      "text      19579 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 306.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLemmText(text):\n",
    "    tokens=word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens=[lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "df['text'] = list(map(getLemmText,df['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStemmText(text):\n",
    "    tokens=word_tokenize(text)\n",
    "    ps = PorterStemmer()\n",
    "    tokens=[ps.stem(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "df['text'] = list(map(getStemmText,df['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13117,)\n",
      "(6462,)\n",
      "6417     MWS\n",
      "1105     MWS\n",
      "12486    EAP\n",
      "18197    EAP\n",
      "11767    MWS\n",
      "3083     HPL\n",
      "18708    MWS\n",
      "363      HPL\n",
      "1834     EAP\n",
      "9430     EAP\n",
      "3333     EAP\n",
      "12083    EAP\n",
      "11220    EAP\n",
      "10994    EAP\n",
      "9497     HPL\n",
      "2521     EAP\n",
      "15693    EAP\n",
      "18184    MWS\n",
      "3766     EAP\n",
      "11602    EAP\n",
      "12878    EAP\n",
      "1875     MWS\n",
      "7325     EAP\n",
      "14449    EAP\n",
      "9677     EAP\n",
      "10188    HPL\n",
      "17630    EAP\n",
      "14790    EAP\n",
      "14211    MWS\n",
      "18499    MWS\n",
      "        ... \n",
      "2747     MWS\n",
      "8035     MWS\n",
      "8924     HPL\n",
      "9488     MWS\n",
      "234      EAP\n",
      "8400     EAP\n",
      "11790    HPL\n",
      "4102     HPL\n",
      "11787    EAP\n",
      "2339     EAP\n",
      "13310    EAP\n",
      "3226     EAP\n",
      "8237     MWS\n",
      "15625    EAP\n",
      "17947    MWS\n",
      "19159    HPL\n",
      "18778    EAP\n",
      "13606    MWS\n",
      "13268    EAP\n",
      "11211    EAP\n",
      "13401    EAP\n",
      "3269     HPL\n",
      "2441     EAP\n",
      "1164     MWS\n",
      "11450    HPL\n",
      "8024     EAP\n",
      "4596     HPL\n",
      "8854     EAP\n",
      "14075    HPL\n",
      "2933     EAP\n",
      "Name: author, Length: 13117, dtype: object\n"
     ]
    }
   ],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(\n",
    " df['text'], df['author'], \n",
    " test_size=0.33, \n",
    " random_state=53)\n",
    "print(xtrain.shape)\n",
    "print(xtest.shape)\n",
    "print(ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIMENSION = 64\n",
    "VOCABULARY_SIZE = 2000\n",
    "MAX_LENGTH = 100\n",
    "OOV_TOK = '<OOV>'\n",
    "TRUNCATE_TYPE = 'post'\n",
    "PADDING_TYPE = 'post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=VOCABULARY_SIZE, oov_token=OOV_TOK)\n",
    "tokenizer.fit_on_texts(list(xtrain) + list(xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 15762\n"
     ]
    }
   ],
   "source": [
    "xtrain_sequences = tokenizer.texts_to_sequences(xtrain)\n",
    "xtest_sequences = tokenizer.texts_to_sequences(xtest)\n",
    "word_index = tokenizer.word_index\n",
    "print('Vocabulary size:', len(word_index))\n",
    "dict(list(word_index.items())[0:10])\n",
    "with open('personal.json', 'w') as json_file:\n",
    "    json.dump(word_index, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27, 6, 113, 2, 424, 94, 29, 1, 40, 37, 110, 1254, 4, 9, 103, 6, 599, 40, 1027, 122, 23, 805, 24, 38, 3, 2, 1, 1, 1, 564, 1, 6, 114, 1755, 1, 19, 8, 528, 4, 1, 57, 1, 920, 69, 8, 54, 1, 117, 44, 2, 826, 3, 744, 1]\n"
     ]
    }
   ],
   "source": [
    "print(xtrain_sequences[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "100\n",
      "[  27    6  113    2  424   94   29    1   40   37  110 1254    4    9\n",
      "  103    6  599   40 1027  122   23  805   24   38    3    2    1    1\n",
      "    1  564    1    6  114 1755    1   19    8  528    4    1   57    1\n",
      "  920   69    8   54    1  117   44    2  826    3  744    1    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n"
     ]
    }
   ],
   "source": [
    "xtrain_pad = sequence.pad_sequences(xtrain_sequences, maxlen=MAX_LENGTH, padding=PADDING_TYPE, truncating=TRUNCATE_TYPE)\n",
    "xtest_pad = sequence.pad_sequences(xtest_sequences, maxlen=MAX_LENGTH, padding=PADDING_TYPE, truncating=TRUNCATE_TYPE)\n",
    "print(len(xtrain_sequences[0]))\n",
    "print(len(xtrain_pad[0]))\n",
    "print(xtrain_pad[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n",
      "[2]\n",
      "[1]\n",
      "(13117, 1)\n"
     ]
    }
   ],
   "source": [
    "label_tokenizer = Tokenizer()\n",
    "label_tokenizer.fit_on_texts(list(ytrain))\n",
    "training_label_seq = np.array(label_tokenizer.texts_to_sequences(ytrain))\n",
    "test_label_seq = np.array(label_tokenizer.texts_to_sequences(ytest))\n",
    "print(training_label_seq[0])\n",
    "print(training_label_seq[1])\n",
    "print(training_label_seq[2])\n",
    "print(training_label_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weari at heart with anxieti which had their origin in the gener <OOV> and decay i <OOV> to the <OOV> fever ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?\n"
     ]
    }
   ],
   "source": [
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "def decode_article(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "print(decode_article(xtrain_pad[11]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\burak\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From c:\\users\\burak\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 64)          1008832   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               66048     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 260       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 1,087,556\n",
      "Trainable params: 1,087,556\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     EMBEDDING_DIMENSION))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(Bidirectional(LSTM(EMBEDDING_DIMENSION, dropout=0.3, recurrent_dropout=0.3)))\n",
    "model.add(Dense(EMBEDDING_DIMENSION, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "model.add(Dense(EMBEDDING_DIMENSION, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13117 samples, validate on 6462 samples\n",
      "Epoch 1/10\n",
      " - 43s - loss: 0.6795 - acc: 0.7509 - val_loss: 0.5855 - val_acc: 0.7629\n",
      "Epoch 2/10\n",
      " - 43s - loss: 0.6265 - acc: 0.7765 - val_loss: 0.5715 - val_acc: 0.7751\n",
      "Epoch 3/10\n",
      " - 45s - loss: 0.6033 - acc: 0.7843 - val_loss: 0.5842 - val_acc: 0.7745\n",
      "Epoch 4/10\n",
      " - 44s - loss: 0.5801 - acc: 0.7974 - val_loss: 0.5734 - val_acc: 0.7767\n",
      "Epoch 5/10\n",
      " - 40s - loss: 0.5534 - acc: 0.8085 - val_loss: 0.6029 - val_acc: 0.7795\n",
      "Epoch 6/10\n",
      " - 40s - loss: 0.5301 - acc: 0.8157 - val_loss: 0.6111 - val_acc: 0.7731\n",
      "Epoch 7/10\n",
      " - 40s - loss: 0.5159 - acc: 0.8228 - val_loss: 0.6022 - val_acc: 0.7744\n",
      "Epoch 8/10\n",
      " - 40s - loss: 0.5099 - acc: 0.8214 - val_loss: 0.6569 - val_acc: 0.7640\n",
      "Epoch 9/10\n",
      " - 41s - loss: 0.4870 - acc: 0.8301 - val_loss: 0.6689 - val_acc: 0.7620\n",
      "Epoch 10/10\n",
      " - 41s - loss: 0.4790 - acc: 0.8363 - val_loss: 0.6896 - val_acc: 0.7724\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "history = model.fit(xtrain_pad, training_label_seq, epochs=num_epochs, validation_data=(xtest_pad, test_label_seq), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
